#!/usr/bin/env python3
"""
æ¸¬è©¦ DeepSeek-OCR æ¨¡å‹è¼‰å…¥
ä½¿ç”¨æ­£ç¢ºçš„ transformers ç‰ˆæœ¬ (4.46.3)
"""

import torch
from transformers import AutoModel, AutoTokenizer
from PIL import Image
import sys

print("=" * 60)
print("DeepSeek-OCR æ¨¡å‹è¼‰å…¥æ¸¬è©¦")
print("=" * 60)
print()

# æª¢æŸ¥ç’°å¢ƒ
print("ã€ç’°å¢ƒæª¢æŸ¥ã€‘")
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

# æª¢æŸ¥ transformers ç‰ˆæœ¬
import transformers
print(f"transformers ç‰ˆæœ¬: {transformers.__version__}")

# æª¢æŸ¥ Flash Attention
try:
    import flash_attn
    print(f"Flash Attention: {flash_attn.__version__}")
except ImportError:
    print("Flash Attention: æœªå®‰è£ï¼ˆå°‡ä½¿ç”¨æ¨™æº–æ¨¡å¼ï¼‰")

print()
print("ã€è¼‰å…¥æ¨¡å‹ã€‘")
print("æ¨¡å‹è·¯å¾‘: ./models/deepseek-ocr")
print("æ­£åœ¨è¼‰å…¥... (é€™å¯èƒ½éœ€è¦ 1-2 åˆ†é˜)")
print()

try:
    # è¼‰å…¥ tokenizer
    print("  [1/3] è¼‰å…¥ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        "./models/deepseek-ocr",
        trust_remote_code=True
    )
    print(f"  âœ“ Tokenizer è¼‰å…¥æˆåŠŸ (è©å½™é‡: {len(tokenizer)})")
    
    # è¼‰å…¥æ¨¡å‹
    print("  [2/3] è¼‰å…¥æ¨¡å‹...")
    model = AutoModel.from_pretrained(
        "./models/deepseek-ocr",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="cuda"
    )
    print(f"  âœ“ æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    
    # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
    print("  [3/3] è¨­å®šè©•ä¼°æ¨¡å¼...")
    model = model.eval()
    print(f"  âœ“ æ¨¡å‹å·²æº–å‚™å°±ç·’")
    
    print()
    print("=" * 60)
    print("âœ“ æ¨¡å‹è¼‰å…¥æ¸¬è©¦é€šéï¼")
    print("=" * 60)
    print()
    print("æ¨¡å‹è³‡è¨Š:")
    print(f"  - è£ç½®: {next(model.parameters()).device}")
    print(f"  - è³‡æ–™é¡å‹: {next(model.parameters()).dtype}")
    print(f"  - åƒæ•¸é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
    
    # æª¢æŸ¥ VRAM ä½¿ç”¨
    if torch.cuda.is_available():
        vram_used = torch.cuda.memory_allocated(0) / 1024**3
        vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  - VRAM ä½¿ç”¨: {vram_used:.2f} GB / {vram_total:.2f} GB ({vram_used/vram_total*100:.1f}%)")
    
    print()
    print("ğŸ‰ æ­å–œï¼æ¨¡å‹å·²æˆåŠŸè¼‰å…¥ï¼Œå¯ä»¥é–‹å§‹ OCR æ¸¬è©¦äº†ï¼")
    print()
    print("ä¸‹ä¸€æ­¥ï¼šåŸ·è¡Œ 'python scripts/test_ocr.py' é€²è¡Œå¯¦éš› OCR æ¸¬è©¦")
    
    sys.exit(0)
    
except Exception as e:
    print()
    print("=" * 60)
    print("âœ— æ¨¡å‹è¼‰å…¥å¤±æ•—")
    print("=" * 60)
    print(f"éŒ¯èª¤è¨Šæ¯: {e}")
    print()
    print("å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
    print("1. ç¢ºèª transformers ç‰ˆæœ¬ç‚º 4.46.3")
    print("2. ç¢ºèªæ¨¡å‹æª”æ¡ˆå®Œæ•´")
    print("3. æª¢æŸ¥ VRAM æ˜¯å¦è¶³å¤ ")
    
    import traceback
    print()
    print("è©³ç´°éŒ¯èª¤:")
    traceback.print_exc()
    
    sys.exit(1)
