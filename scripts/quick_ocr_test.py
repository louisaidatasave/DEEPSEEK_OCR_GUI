#!/usr/bin/env python3
"""
å¿«é€Ÿ OCR æ¸¬è©¦è…³æœ¬
æ¸¬è©¦ DeepSeek-OCR å°‡åœ–ç‰‡è½‰æ›ç‚º Markdown
"""

import torch
from transformers import AutoModel, AutoTokenizer
from PIL import Image
import time

print("=" * 60)
print("DeepSeek-OCR å¿«é€Ÿæ¸¬è©¦")
print("=" * 60)
print()

# æ¸¬è©¦åœ–ç‰‡è·¯å¾‘
image_path = r"D:\NAS_Share\Data\08_Programming_Data(ç¨‹å¼è³‡æ–™)\07_DEEPSEEK_OCR\è¢å¹•æ“·å–ç•«é¢ 2025-11-01 145249.png"

print(f"æ¸¬è©¦åœ–ç‰‡: {image_path}")
print()

# è¼‰å…¥åœ–ç‰‡
print("ã€æ­¥é©Ÿ 1ã€‘è¼‰å…¥åœ–ç‰‡...")
try:
    image = Image.open(image_path).convert("RGB")
    print(f"âœ“ åœ–ç‰‡è¼‰å…¥æˆåŠŸ")
    print(f"  å°ºå¯¸: {image.size[0]} x {image.size[1]} px")
    print(f"  æ ¼å¼: {image.format if hasattr(image, 'format') else 'PNG'}")
except Exception as e:
    print(f"âœ— åœ–ç‰‡è¼‰å…¥å¤±æ•—: {e}")
    exit(1)

print()

# è¼‰å…¥æ¨¡å‹
print("ã€æ­¥é©Ÿ 2ã€‘è¼‰å…¥æ¨¡å‹...")
try:
    model_path = "./models/deepseek-ocr"
    
    print("  è¼‰å…¥ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    print("  è¼‰å…¥æ¨¡å‹...")
    model = AutoModel.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="cuda"
    )
    model = model.eval()
    
    print(f"âœ“ æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    
    # é¡¯ç¤º VRAM ä½¿ç”¨
    if torch.cuda.is_available():
        vram_used = torch.cuda.memory_allocated(0) / 1024**3
        print(f"  VRAM ä½¿ç”¨: {vram_used:.2f} GB")
    
except Exception as e:
    print(f"âœ— æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    exit(1)

print()

# åŸ·è¡Œ OCR
print("ã€æ­¥é©Ÿ 3ã€‘åŸ·è¡Œ OCR...")
print("  æç¤ºè©: <image>\\n<|grounding|>Convert the document to markdown.")
print("  æ¨¡å¼: Base (1024x1024)")
print("  è™•ç†ä¸­... (é€™å¯èƒ½éœ€è¦ 10-30 ç§’)")
print()

try:
    # ä½¿ç”¨å®˜æ–¹æ¨è–¦çš„æç¤ºè©
    prompt = "<image>\n<|grounding|>Convert the document to markdown. "
    
    # è¨˜éŒ„é–‹å§‹æ™‚é–“
    start_time = time.time()
    
    # ä½¿ç”¨æ¨¡å‹çš„ infer æ–¹æ³•ï¼ˆå®˜æ–¹æ–¹å¼ï¼‰
    # Base æ¨¡å¼: base_size = 1024, image_size = 1024, crop_mode = False
    result = model.infer(
        tokenizer,
        prompt=prompt,
        image_file=image_path,
        output_path="outputs",
        base_size=1024,
        image_size=1024,
        crop_mode=False,
        save_results=True,
        test_compress=True
    )
    
    # è¨˜éŒ„çµæŸæ™‚é–“
    end_time = time.time()
    processing_time = end_time - start_time
    
    print(f"âœ“ OCR å®Œæˆï¼")
    print(f"  è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
    
    # é¡¯ç¤º VRAM ä½¿ç”¨
    if torch.cuda.is_available():
        vram_used = torch.cuda.memory_allocated(0) / 1024**3
        vram_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"  VRAM ä½¿ç”¨: {vram_used:.2f} GB / {vram_total:.2f} GB ({vram_used/vram_total*100:.1f}%)")
    
    print()
    print("=" * 60)
    print("OCR çµæœï¼ˆMarkdown æ ¼å¼ï¼‰")
    print("=" * 60)
    print()
    print(result)
    print()
    print("=" * 60)
    
    # å„²å­˜çµæœ
    output_file = "outputs/ocr_result.md"
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(result)
        print(f"\nâœ“ çµæœå·²å„²å­˜è‡³: {output_file}")
    except Exception as e:
        print(f"\nâš  ç„¡æ³•å„²å­˜çµæœ: {e}")
    
    print()
    print("ğŸ‰ æ¸¬è©¦æˆåŠŸï¼")
    
except Exception as e:
    print(f"âœ— OCR åŸ·è¡Œå¤±æ•—: {e}")
    print()
    print("è©³ç´°éŒ¯èª¤:")
    import traceback
    traceback.print_exc()
    exit(1)
